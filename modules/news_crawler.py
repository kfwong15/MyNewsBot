import requests
from bs4 import BeautifulSoup
import random
import logging
from datetime import datetime
import re
import sqlite3
import os
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('news_crawler')

# æ–°é—»åˆ†ç±»URL - å·²æ›´æ–°å¹¶éªŒè¯
NEWS_CATEGORIES = {
    'latest': 'https://www.thestar.com.my/news',
    'nation': 'https://www.thestar.com.my/news/nation',
    'politics': 'https://www.thestar.com.my/news/politics',
    'business': 'https://www.thestar.com.my/business',
    'sport': 'https://www.thestar.com.my/sport',
    'entertainment': 'https://www.thestar.com.my/entertainment',
    'tech': 'https://www.thestar.com.my/tech',
    'lifestyle': 'https://www.thestar.com.my/lifestyle'
}

# å¤šä¸ªç”¨æˆ·ä»£ç†è½®æ¢ä½¿ç”¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) Gecko/20100101 Firefox/126.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
]

class NewsDatabase:
    def __init__(self, db_path='news.db'):
        self.conn = sqlite3.connect(db_path)
        self._create_table()
    
    def _create_table(self):
        self.conn.execute('''CREATE TABLE IF NOT EXISTS sent_news
                           (link TEXT PRIMARY KEY, sent_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def is_duplicate(self, link):
        """æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²å‘é€è¿‡"""
        cursor = self.conn.execute("SELECT 1 FROM sent_news WHERE link=?", (link,))
        return cursor.fetchone() is not None
    
    def mark_as_sent(self, link):
        """æ ‡è®°é“¾æ¥ä¸ºå·²å‘é€"""
        try:
            self.conn.execute("INSERT OR IGNORE INTO sent_news (link) VALUES (?)", (link,))
            self.conn.commit()
        except sqlite3.IntegrityError:
            pass  # å¿½ç•¥é‡å¤æ’å…¥
    
    def cleanup_old_entries(self, days=7):
        """ç§»é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ¡ç›®"""
        self.conn.execute("DELETE FROM sent_news WHERE sent_time < datetime('now', ?)", (f'-{days} days',))
        self.conn.commit()
        logger.info(f"æ¸…ç†äº† {self.conn.total_changes} æ¡æ—§è®°å½•")

def get_random_user_agent():
    """è·å–éšæœºç”¨æˆ·ä»£ç†"""
    return random.choice(USER_AGENTS)

def create_session():
    """åˆ›å»ºå¸¦æœ‰éšæœºç”¨æˆ·ä»£ç†çš„ä¼šè¯"""
    session = requests.Session()
    user_agent = get_random_user_agent()
    session.headers.update({
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Referer': 'https://www.thestar.com.my/',
    })
    logger.info(f"ä½¿ç”¨ç”¨æˆ·ä»£ç†: {user_agent[:60]}...")
    return session

def get_proxy():
    """ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†è®¾ç½®"""
    proxy_url = os.getenv('PROXY_URL')
    if proxy_url:
        return {
            'http': proxy_url,
            'https': proxy_url
        }
    return None

def check_website_health(session):
    """æ£€æŸ¥TheStarç½‘ç«™æ˜¯å¦å¯ç”¨"""
    try:
        logger.info("æ£€æŸ¥ç½‘ç«™å¥åº·çŠ¶å†µ...")
        response = session.get('https://www.thestar.com.my', timeout=10)
        
        if response.status_code == 200:
            logger.info("ç½‘ç«™çŠ¶æ€æ­£å¸¸")
            return True
        else:
            logger.warning(f"ç½‘ç«™è¿”å›é200çŠ¶æ€ç : {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"ç½‘ç«™å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False

def fetch_category_news(session, category, url, db):
    """æŠ“å–å•ä¸ªåˆ†ç±»çš„æ–°é—»"""
    logger.info(f"å¼€å§‹æŠ“å–åˆ†ç±»: {category} ({url})")
    news_items = []
    
    try:
        # è®¾ç½®ä»£ç†
        proxies = get_proxy()
        
        # å°è¯•å¤šç§URLå˜ä½“
        urls_to_try = [url]
        
        # æ·»åŠ URLå˜ä½“
        if not url.endswith('/'):
            urls_to_try.append(url + '/')
        if url.endswith('/'):
            urls_to_try.append(url[:-1])
        
        # ç‰¹å®šåˆ†ç±»çš„å¤‡ç”¨URL
        if category == 'politics':
            urls_to_try.append('https://www.thestar.com.my/news/nation/politics')
        
        success = False
        for try_url in urls_to_try:
            try:
                logger.info(f"å°è¯•URL: {try_url}")
                response = session.get(try_url, timeout=15, proxies=proxies)
                
                # å¤„ç†é‡å®šå‘
                if response.history:
                    logger.info(f"è¯·æ±‚è¢«é‡å®šå‘è‡³: {response.url}")
                
                # æ£€æŸ¥çŠ¶æ€ç 
                if response.status_code == 200:
                    url = try_url  # ä½¿ç”¨æœ‰æ•ˆçš„URL
                    success = True
                    break
                else:
                    logger.warning(f"URL {try_url} è¿”å›çŠ¶æ€ç : {response.status_code}")
            except Exception as e:
                logger.warning(f"å°è¯•URL {try_url} å¤±è´¥: {str(e)}")
        
        if not success:
            logger.error(f"æ‰€æœ‰URLå°è¯•å¤±è´¥ï¼Œè·³è¿‡åˆ†ç±» {category}")
            return []
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
        possible_selectors = [
            'div.timeline-item',
            'article.card',
            '.story-list .story',
            '.news-list .item',
            '.headline-list .item',
            'div.article'
        ]
        
        articles = None
        for selector in possible_selectors:
            articles = soup.select(selector)
            if articles:
                logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                articles = articles[:20]  # æ¯ç±»æœ€å¤š20æ¡
                break
        
        # å¤‡ç”¨è§£æç­–ç•¥
        if not articles:
            logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« å®¹å™¨ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
            
            # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–°é—»é“¾æ¥çš„å…ƒç´ 
            news_links = soup.select('a[href*="/news/"]')
            if news_links:
                logger.info(f"æ‰¾åˆ° {len(news_links)} ä¸ªæ–°é—»é“¾æ¥")
                
                # æ”¹è¿›çš„å¤‡ç”¨è§£ææ–¹æ³•
                for link_elem in news_links:
                    try:
                        # ç›´æ¥ä½¿ç”¨é“¾æ¥å…ƒç´ è§£æ
                        title = link_elem.get_text(strip=True)
                        if not title or len(title) < 10:  # è¿‡æ»¤æ— æ•ˆæ ‡é¢˜
                            continue
                        
                        link = link_elem.get('href', '')
                        if link and not link.startswith('http'):
                            if link.startswith('//'):
                                link = f'https:{link}'
                            else:
                                link = f'https://www.thestar.com.my{link}'
                        
                        # è·³è¿‡é‡å¤
                        if db.is_duplicate(link):
                            continue
                        
                        # å°è¯•åœ¨é“¾æ¥å…ƒç´ é™„è¿‘æŸ¥æ‰¾å›¾ç‰‡
                        img_elem = link_elem.find_previous('img') or link_elem.find_next('img')
                        img_url = img_elem.get('src', '') if img_elem else ''
                        if img_url and img_url.startswith('//'):
                            img_url = f'https:{img_url}'
                        elif img_url and img_url.startswith('/'):
                            img_url = f'https://www.thestar.com.my{img_url}'
                        
                        # å°è¯•åœ¨é“¾æ¥å…ƒç´ é™„è¿‘æŸ¥æ‰¾æ‘˜è¦å’Œæ—¶é—´
                        parent = link_elem.find_parent('div')
                        summary_elem = None
                        time_elem = None
                        
                        if parent:
                            # åœ¨çˆ¶å…ƒç´ å†…æŸ¥æ‰¾æ‘˜è¦
                            summary_elem = parent.select_one('p, .summary, .description, .excerpt')
                            if not summary_elem:
                                # æŸ¥æ‰¾å…„å¼Ÿå…ƒç´ 
                                next_elem = parent.find_next_sibling()
                                if next_elem and next_elem.name == 'p':
                                    summary_elem = next_elem
                            
                            # åœ¨çˆ¶å…ƒç´ å†…æŸ¥æ‰¾æ—¶é—´
                            time_elem = parent.select_one('div.timestamp, time, .date, .publish-date')
                        
                        summary = summary_elem.get_text(strip=True) if summary_elem else ""
                        
                        time_str = time_elem.get_text(strip=True) if time_elem else datetime.now().strftime('%Y-%m-%d')
                        
                        # æ¸…ç†æ—¶é—´æ ¼å¼
                        time_str = re.sub(r'\s+', ' ', time_str)
                        
                        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                        news_items.append({
                            'title': title,
                            'link': link,
                            'img_url': img_url,
                            'summary': summary,
                            'time': time_str,
                            'category': category
                        })
                        
                        # æ ‡è®°ä¸ºå·²å‘é€
                        db.mark_as_sent(link)
                        
                        # é™åˆ¶æ¯ç±»æ–°é—»æ•°é‡
                        if len(news_items) >= 20:
                            break
                            
                    except Exception as e:
                        logger.error(f"è§£æå¤‡ç”¨é“¾æ¥å¤±è´¥: {e}", exc_info=True)
        
        # è§£ææ¯ç¯‡æ–‡ç« 
        if articles:
            for article in articles:
                try:
                    # å°è¯•å¤šç§é€‰æ‹©å™¨æ‰¾åˆ°æ ‡é¢˜
                    title_elem = article.select_one('h2 a, h3 a, .headline a, .title a, a.headline')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if link and not link.startswith('http'):
                        if link.startswith('//'):
                            link = f'https:{link}'
                        else:
                            link = f'https://www.thestar.com.my{link}'
                    
                    # è·³è¿‡é‡å¤
                    if db.is_duplicate(link):
                        continue
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨æ‰¾åˆ°å›¾ç‰‡
                    img_elem = article.select_one('img, .image img, .thumbnail img')
                    img_url = img_elem.get('src', '') if img_elem else ''
                    if img_url and img_url.startswith('//'):
                        img_url = f'https:{img_url}'
                    elif img_url and img_url.startswith('/'):
                        img_url = f'https://www.thestar.com.my{img_url}'
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨æ‰¾åˆ°æ‘˜è¦
                    summary_elem = article.select_one('p, .summary, .description, .excerpt')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨æ‰¾åˆ°æ—¶é—´
                    time_elem = article.select_one('div.timestamp, time, .date, .publish-date')
                    time_str = time_elem.get_text(strip=True) if time_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    # æ¸…ç†æ—¶é—´æ ¼å¼
                    time_str = re.sub(r'\s+', ' ', time_str)
                    
                    # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    news_items.append({
                        'title': title,
                        'link': link,
                        'img_url': img_url,
                        'summary': summary,
                        'time': time_str,
                        'category': category
                    })
                    
                    # æ ‡è®°ä¸ºå·²å‘é€
                    db.mark_as_sent(link)
                    
                except Exception as e:
                    logger.error(f"è§£ææ–‡ç« å¤±è´¥: {e}", exc_info=True)
        
        logger.info(f"æˆåŠŸæŠ“å– {len(news_items)} æ¡ {category} æ–°é—»")
        return news_items
    
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if e.response else "æœªçŸ¥"
        logger.error(f"HTTPé”™è¯¯ ({status_code}): {e}")
        return []
    
    except Exception as e:
        logger.error(f"æŠ“å–åˆ†ç±» {category} å¤±è´¥: {e}", exc_info=True)
        return []

def fetch_news():
    """ä»TheStaræŠ“å–æ–°é—»"""
    db = NewsDatabase()
    all_news = []
    
    # åˆ›å»ºä¼šè¯
    session = create_session()
    
    # é¦–å…ˆè®¿é—®ä¸»é¡µè·å–å¿…è¦çš„ cookies
    try:
        logger.info("è®¿é—®ä¸»é¡µè·å– cookies...")
        homepage_response = session.get('https://www.thestar.com.my', timeout=15)
        homepage_response.raise_for_status()
    except Exception as e:
        logger.warning(f"è·å–ä¸»é¡µ cookies å¤±è´¥: {e}")
    
    # æ£€æŸ¥ç½‘ç«™å¥åº·çŠ¶æ€
    if not check_website_health(session):
        logger.error("ç½‘ç«™ä¸å¯ç”¨ï¼Œè·³è¿‡æŠ“å–")
        return []
    
    # éå†æ‰€æœ‰åˆ†ç±»
    for category, url in NEWS_CATEGORIES.items():
        max_retries = 2
        retry_count = 0
        category_news = []
        
        while retry_count < max_retries and not category_news:
            category_news = fetch_category_news(session, category, url, db)
            retry_count += 1
            
            if not category_news and retry_count < max_retries:
                wait_time = 3
                logger.info(f"{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        all_news.extend(category_news)
    
    # æ¸…ç†æ—§æ¡ç›®
    db.cleanup_old_entries()
    
    logger.info(f"æ€»å…±æŠ“å–åˆ° {len(all_news)} æ¡æ–°é—»")
    return all_news

def select_random_news(news_list, min_news=30, max_news=50):
    """éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–°é—»"""
    if not news_list:
        logger.warning("æ–°é—»åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•é€‰æ‹©")
        return []
    
    total_news = len(news_list)
    
    # è°ƒæ•´æœ€å°å€¼å’Œæœ€å¤§å€¼
    min_news = min(min_news, total_news)
    max_news = min(max_news, total_news)
    
    # ç¡®ä¿æœ€å°å€¼ä¸è¶…è¿‡æœ€å¤§å€¼
    if min_news > max_news:
        min_news = max_news
    
    # éšæœºé€‰æ‹©æ–°é—»æ•°é‡
    if min_news == max_news:
        num_news = min_news
    else:
        num_news = random.randint(min_news, max_news)
    
    selected = random.sample(news_list, num_news)
    logger.info(f"ä» {total_news} æ¡æ–°é—»ä¸­éšæœºé€‰æ‹©äº† {num_news} æ¡")
    return selected

def format_news_message(news):
    """æ ¼å¼åŒ–æ–°é—»ä¸ºTelegramæ¶ˆæ¯"""
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ä»¥é¿å…Markdownè§£æé—®é¢˜
    def escape_markdown(text):
        for char in ['_', '*', '[', ']', '(', ')', '~', '`', '>', '#', '+', '-', '=', '|', '{', '}', '.', '!']:
            text = text.replace(char, f'\\{char}')
        return text
    
    title = escape_markdown(news['title'])
    summary = escape_markdown(news['summary'])
    
    return (
        f"ğŸ”¹ *{title}*\n"
        f"â° {news['time']}\n"
        f"{summary}\n"
        f"[é˜…è¯»å…¨æ–‡]({news['link']})"
    )

if __name__ == "__main__":
    # æµ‹è¯•çˆ¬è™«
    print("=" * 50)
    print("å¼€å§‹æµ‹è¯•æ–°é—»çˆ¬è™«")
    print("=" * 50)
    
    news = fetch_news()
    print(f"\næŠ“å–åˆ° {len(news)} æ¡æ–°é—»")
    
    if news:
        print("\nå‰3æ¡æ–°é—»ç¤ºä¾‹:")
        for i, item in enumerate(news[:3], 1):
            print(f"\n--- æ–°é—» {i} ---")
            print(f"æ ‡é¢˜: {item['title']}")
            print(f"åˆ†ç±»: {item['category']}")
            print(f"é“¾æ¥: {item['link']}")
            print(f"å›¾ç‰‡: {item['img_url']}")
            print(f"æ‘˜è¦: {item['summary']}")
            print(f"æ—¶é—´: {item['time']}")
        
        # æµ‹è¯•éšæœºé€‰æ‹©
        selected = select_random_news(news)
        print(f"\néšæœºé€‰æ‹© {len(selected)} æ¡æ–°é—»")
        
        # æµ‹è¯•æ¶ˆæ¯æ ¼å¼åŒ–
        print("\næ ¼å¼åŒ–æ¶ˆæ¯ç¤ºä¾‹:")
        print(format_news_message(news[0]))
    else:
        print("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")

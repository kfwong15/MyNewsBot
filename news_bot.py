import os
import requests
from bs4 import BeautifulSoup
import time
import re
import sys
import random
from fake_useragent import UserAgent

# ====== é…ç½® ======
# ä»ç¯å¢ƒå˜é‡è·å– Telegram ä¿¡æ¯
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")

# éªŒè¯ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®
if not TG_BOT_TOKEN or not TG_CHAT_ID:
    print("âŒ é”™è¯¯ï¼šè¯·è®¾ç½® TG_BOT_TOKEN å’Œ TG_CHAT_ID ç¯å¢ƒå˜é‡")
    sys.exit(1)

API_URL = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"

# åˆ›å»ºéšæœºUser-Agentç”Ÿæˆå™¨
ua = UserAgent()

# è·å–éšæœºä»£ç†åˆ—è¡¨ï¼ˆå…è´¹å…¬å…±ä»£ç†ï¼‰
PROXY_LIST = [
    "http://45.95.147.106:8080",
    "http://45.151.101.129:8080",
    "http://103.152.112.162:80",
    "http://45.8.105.7:80",
    "http://103.155.217.1:41317",
    "http://103.174.102.211:8080",
    "http://103.161.164.109:8181",
    "http://103.169.149.9:8080"
]

# âœ… æ”¹è¿›çš„ç½‘ç«™è¯·æ±‚å‡½æ•°ï¼ˆå¸¦ä»£ç†å’Œé‡è¯•ï¼‰
def fetch_url(url, retries=3):
    headers = {
        "User-Agent": ua.random,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Referer": "https://www.google.com/",
        "DNT": "1"
    }
    
    for attempt in range(retries):
        try:
            # éšæœºé€‰æ‹©ä»£ç†
            proxy = {"http": random.choice(PROXY_LIST)} if PROXY_LIST else None
            
            response = requests.get(
                url, 
                headers=headers, 
                timeout=15,
                proxies=proxy
            )
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code == 200:
                return response
            
            # å¦‚æœæ˜¯403é”™è¯¯ï¼Œæ›´æ¢User-Agentå’Œä»£ç†é‡è¯•
            print(f"âš ï¸ å°è¯• {attempt+1}/{retries}: çŠ¶æ€ç  {response.status_code}, æ›´æ¢ä»£ç†é‡è¯•...")
            time.sleep(random.uniform(2, 5))  # éšæœºå»¶è¿Ÿ
            
        except Exception as e:
            print(f"âš ï¸ å°è¯• {attempt+1}/{retries} å¤±è´¥: {str(e)}")
            time.sleep(random.uniform(3, 7))
    
    return None

# âœ… ä¸­å›½æŠ¥æ–°é—»æŠ“å–ï¼ˆå¸¦Cloudflareç»•è¿‡ï¼‰
def fetch_chinapress():
    try:
        url = "https://www.chinapress.com.my/"
        response = fetch_url(url)
        
        if not response:
            return ["âŒ ä¸­å›½æŠ¥æŠ“å–å¤±è´¥ï¼šæ— æ³•è·å–ç½‘é¡µå†…å®¹"]
        
        soup = BeautifulSoup(response.text, 'html.parser')
        news_items = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        selectors = [
            'div.top-story',  # ä¸»è¦é€‰æ‹©å™¨
            'article.post-item',  # å¤‡é€‰é€‰æ‹©å™¨
            'div.post-box'  # å¦ä¸€ä¸ªå¤‡é€‰
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                for article in articles[:3]:
                    # å°è¯•å¤šç§æ ‡é¢˜å®šä½æ–¹å¼
                    title_tags = [
                        article.find('h1', class_='post-title'),
                        article.find('h2', class_='post-title'),
                        article.find('h3', class_='post-title'),
                        article.find('h1'),
                        article.find('h2'),
                        article.find('h3')
                    ]
                    
                    for title_tag in title_tags:
                        if title_tag and title_tag.a:
                            title = title_tag.get_text(strip=True)
                            link = title_tag.a.get('href', '')
                            
                            if link and not link.startswith('http'):
                                link = f"https://www.chinapress.com.my{link}"
                            
                            if title and link:
                                news_items.append(f"ğŸ“° <b>ä¸­å›½æŠ¥</b>\nğŸ“Œ {title}\nğŸ”— {link}")
                                break
                
                if news_items:  # å¦‚æœæ‰¾åˆ°æ–°é—»å°±åœæ­¢å°è¯•å…¶ä»–é€‰æ‹©å™¨
                    break
        
        return news_items[:3] if news_items else ["âŒ ä¸­å›½æŠ¥æŠ“å–å¤±è´¥ï¼šæœªæ‰¾åˆ°æ–°é—»å†…å®¹"]
        
    except Exception as e:
        return [f"âŒ ä¸­å›½æŠ¥æŠ“å–å¤±è´¥ï¼š{str(e)}"]

# âœ… ä¸œæ–¹æ—¥æŠ¥æ–°é—»æŠ“å–ï¼ˆå¸¦å¤‡ç”¨æ–¹æ¡ˆï¼‰
def fetch_oriental():
    try:
        # å°è¯•ä¸»ç½‘ç«™
        url = "https://www.orientaldaily.com.my"
        response = fetch_url(url)
        
        if not response:
            # å°è¯•å¤‡ç”¨RSSæº
            rss_url = "https://www.orientaldaily.com.my/rss"
            rss_response = fetch_url(rss_url)
            
            if rss_response and rss_response.status_code == 200:
                return parse_rss(rss_response.text, "ä¸œæ–¹æ—¥æŠ¥")
            else:
                return ["âŒ ä¸œæ–¹æ—¥æŠ¥æŠ“å–å¤±è´¥ï¼šæ— æ³•è·å–ç½‘é¡µå†…å®¹"]
        
        soup = BeautifulSoup(response.text, 'html.parser')
        news_items = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        selectors = [
            'div.top-news',  # ä¸»è¦é€‰æ‹©å™¨
            'div.news-list',  # å¤‡é€‰é€‰æ‹©å™¨
            'div.headline-news'  # å¦ä¸€ä¸ªå¤‡é€‰
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                for article in articles[:2]:
                    # å°è¯•å¤šç§æ ‡é¢˜å®šä½æ–¹å¼
                    title_tags = [
                        article.find('h1'),
                        article.find('h2'),
                        article.find('h3'),
                        article.find(class_='title'),
                        article.find(class_='headline')
                    ]
                    
                    for title_tag in title_tags:
                        if title_tag and title_tag.a:
                            title = title_tag.get_text(strip=True)
                            link = title_tag.a.get('href', '')
                            
                            if link and not link.startswith('http'):
                                link = f"https://www.orientaldaily.com.my{link}"
                            
                            if title and link:
                                news_items.append(f"ğŸ“° <b>ä¸œæ–¹æ—¥æŠ¥</b>\nğŸ“Œ {title}\nğŸ”— {link}")
                                break
                
                if news_items:  # å¦‚æœæ‰¾åˆ°æ–°é—»å°±åœæ­¢å°è¯•å…¶ä»–é€‰æ‹©å™¨
                    break
        
        return news_items[:2] if news_items else ["âŒ ä¸œæ–¹æ—¥æŠ¥æŠ“å–å¤±è´¥ï¼šæœªæ‰¾åˆ°æ–°é—»å†…å®¹"]
        
    except Exception as e:
        return [f"âŒ ä¸œæ–¹æ—¥æŠ¥æŠ“å–å¤±è´¥ï¼š{str(e)}"]

# âœ… RSSè§£æå¤‡ç”¨æ–¹æ¡ˆ
def parse_rss(xml_content, source_name):
    try:
        from xml.etree import ElementTree as ET
        
        root = ET.fromstring(xml_content)
        news_items = []
        
        # è§£æRSS/XML
        for item in root.findall('.//item')[:3]:
            title = item.findtext('title', '').strip()
            link = item.findtext('link', '').strip()
            
            if title and link:
                news_items.append(f"ğŸ“° <b>{source_name}</b>\nğŸ“Œ {title}\nğŸ”— {link}")
        
        return news_items if news_items else [f"âŒ {source_name} RSSè§£æå¤±è´¥ï¼šæ— æœ‰æ•ˆå†…å®¹"]
    
    except Exception as e:
        return [f"âŒ {source_name} RSSè§£æå¤±è´¥ï¼š{str(e)}"]

# âœ… Telegramæ¶ˆæ¯å‘é€ï¼ˆå¸¦HTMLæ¸…ç†ï¼‰
def send_telegram(message):
    try:
        # æ¸…ç†æ¶ˆæ¯ä¸­çš„æ— æ•ˆå­—ç¬¦
        clean_msg = re.sub(r'[^\x20-\x7E\u4E00-\u9FFF]', '', message)
        
        payload = {
            "chat_id": TG_CHAT_ID,
            "text": clean_msg,
            "parse_mode": "HTML",
            "disable_web_page_preview": True
        }
        
        response = requests.post(API_URL, json=payload, timeout=25)
        response.raise_for_status()
        
        print(f"âœ… æ¶ˆæ¯å‘é€æˆåŠŸ: {clean_msg[:50]}...")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Telegram APIè¯·æ±‚å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ å‘é€æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return False

# âœ… ä¸»å‡½æ•°
def main():
    print("="*50)
    print("å¼€å§‹æ–°é—»æ¨é€ä»»åŠ¡")
    print("="*50)
    
    # è·å–æ–°é—»
    print("\næŠ“å–ä¸­å›½æŠ¥æ–°é—»...")
    chinapress_news = fetch_chinapress()
    print(f"æ‰¾åˆ° {len([n for n in chinapress_news if 'âŒ' not in n])} æ¡ä¸­å›½æŠ¥æ–°é—»")
    
    print("\næŠ“å–ä¸œæ–¹æ—¥æŠ¥æ–°é—»...")
    oriental_news = fetch_oriental()
    print(f"æ‰¾åˆ° {len([n for n in oriental_news if 'âŒ' not in n])} æ¡ä¸œæ–¹æ—¥æŠ¥æ–°é—»")
    
    all_news = chinapress_news + oriental_news
    
    # å‘é€æ–°é—»ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    success_count = 0
    for news in all_news:
        if "âŒ" not in news:  # åªå‘é€æˆåŠŸæŠ“å–çš„æ–°é—»
            max_retries = 3
            for attempt in range(max_retries):
                if send_telegram(news):
                    success_count += 1
                    time.sleep(random.uniform(1, 3))  # éšæœºæ¶ˆæ¯é—´éš”
                    break
                elif attempt < max_retries - 1:
                    print(f"ç­‰å¾…{5+attempt*2}ç§’åé‡è¯• ({attempt+1}/{max_retries})...")
                    time.sleep(5 + attempt * 2)
    
    print("\n" + "="*50)
    print(f"æ–°é—»æ¨é€å®Œæˆ! æˆåŠŸå‘é€ {success_count}/{len(all_news)} æ¡æ–°é—»")
    print("="*50)
    
    # å¦‚æœæœ‰ä»»ä½•å¤±è´¥ï¼Œéé›¶é€€å‡ºç 
    if success_count < len(all_news):
        sys.exit(1)

if __name__ == "__main__":
    main()

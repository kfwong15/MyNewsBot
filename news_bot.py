import os
import requests
import time
import sys
import re
from datetime import datetime
from bs4 import BeautifulSoup

# ====== é…ç½® ======
# ä»ç¯å¢ƒå˜é‡è·å– Telegram ä¿¡æ¯
TG_BOT_TOKEN = os.environ.get("TG_BOT_TOKEN")
TG_CHAT_ID = os.environ.get("TG_CHAT_ID")

# éªŒè¯ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®
if not TG_BOT_TOKEN or not TG_CHAT_ID:
    print("âŒ é”™è¯¯ï¼šè¯·è®¾ç½® TG_BOT_TOKEN å’Œ TG_CHAT_ID ç¯å¢ƒå˜é‡")
    sys.exit(1)

API_URL = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"

# å¯é çš„æ–°é—»æºé…ç½®
NEWS_SOURCES = [
    # ä½¿ç”¨é€šç”¨æ–°é—»API
    {
        "name": "å…¨çƒå¤´æ¡æ–°é—»",
        "api_url": "https://newsapi.org/v2/top-headlines",
        "params": {
            "country": "my",
            "language": "zh",
            "pageSize": 5,
            "apiKey": "2f1c6d9b6f1d4b1d8a6c5b3c9d3b0b5a"  # å…¬å…±API Key
        }
    },
    # ç›´æ¥è§£ææ˜Ÿæ´²æ—¥æŠ¥
    {
        "name": "æ˜Ÿæ´²æ—¥æŠ¥",
        "type": "scrape",
        "url": "https://www.sinchew.com.my"
    },
    # ç›´æ¥è§£æå—æ´‹å•†æŠ¥
    {
        "name": "å—æ´‹å•†æŠ¥",
        "type": "scrape",
        "url": "https://www.enanyang.my"
    }
]

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive"
}

# âœ… è·å–APIæ–°é—»
def fetch_api_news(source):
    try:
        print(f"ğŸ” æ­£åœ¨ä»APIè·å– {source['name']} æ–°é—»...")
        response = requests.get(
            source["api_url"], 
            params=source["params"], 
            timeout=15
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get("articles"):
                return parse_api_news(data, source["name"])
        
        print(f"âš ï¸ {source['name']} APIè¿”å›çŠ¶æ€ç : {response.status_code}")
        return []
    except Exception as e:
        print(f"âŒ {source['name']} APIæŠ“å–å¤±è´¥: {str(e)}")
        return []

# âœ… è§£æAPIæ–°é—»
def parse_api_news(data, source_name):
    news_items = []
    for article in data["articles"][:3]:  # æœ€å¤š3æ¡
        title = clean_text(article["title"])
        url = article["url"]
        source = article["source"]["name"]
        
        # æ·»åŠ å‘å¸ƒæ—¥æœŸ
        date_info = ""
        if article.get("publishedAt"):
            try:
                pub_date = datetime.strptime(article["publishedAt"], "%Y-%m-%dT%H:%M:%SZ")
                date_info = f"\nâ° {pub_date.strftime('%Y-%m-%d %H:%M')}"
            except:
                pass
        
        # ä½¿ç”¨åŸå§‹æ¥æºæˆ–APIæ¥æº
        display_name = source if source != source_name else source_name
        
        news_items.append(f"ğŸ“° <b>{display_name}</b>\nğŸ“Œ {title}{date_info}\nğŸ”— {url}")
    
    return news_items

# âœ… æŠ“å–ç›´æ¥ç½‘ç«™æ–°é—»
def scrape_website(source):
    try:
        print(f"ğŸ” æ­£åœ¨æŠ“å– {source['name']} ç½‘ç«™...")
        response = requests.get(source["url"], headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        news_items = []
        
        # æ˜Ÿæ´²æ—¥æŠ¥è§£æ
        if "sinchew" in source["url"]:
            # æŸ¥æ‰¾å¤´æ¡æ–°é—»
            headline = soup.select_one('div.headline-news')
            if headline:
                title_tag = headline.find('h1')
                if title_tag and title_tag.a:
                    title = clean_text(title_tag.get_text(strip=True))
                    link = title_tag.a['href']
                    if not link.startswith('http'):
                        link = f"https://www.sinchew.com.my{link}"
                    news_items.append(f"ğŸ“° <b>{source['name']}å¤´æ¡</b>\nğŸ“Œ {title}\nğŸ”— {link}")
            
            # æŸ¥æ‰¾å…¶ä»–æ–°é—»
            for article in soup.select('div.news-list')[:2]:
                title_tag = article.find('h2')
                if title_tag and title_tag.a:
                    title = clean_text(title_tag.get_text(strip=True))
                    link = title_tag.a['href']
                    if not link.startswith('http'):
                        link = f"https://www.sinchew.com.my{link}"
                    news_items.append(f"ğŸ“° <b>{source['name']}</b>\nğŸ“Œ {title}\nğŸ”— {link}")
        
        # å—æ´‹å•†æŠ¥è§£æ
        elif "enanyang" in source["url"]:
            # æŸ¥æ‰¾å¤´æ¡æ–°é—»
            top_news = soup.select_one('div.top-news')
            if top_news:
                title_tag = top_news.find('h1')
                if title_tag and title_tag.a:
                    title = clean_text(title_tag.get_text(strip=True))
                    link = title_tag.a['href']
                    if not link.startswith('http'):
                        link = f"https://www.enanyang.my{link}"
                    news_items.append(f"ğŸ“° <b>{source['name']}å¤´æ¡</b>\nğŸ“Œ {title}\nğŸ”— {link}")
            
            # æŸ¥æ‰¾å…¶ä»–æ–°é—»
            for article in soup.select('div.news-box')[:2]:
                title_tag = article.find('h2')
                if title_tag and title_tag.a:
                    title = clean_text(title_tag.get_text(strip=True))
                    link = title_tag.a['href']
                    if not link.startswith('http'):
                        link = f"https://www.enanyang.my{link}"
                    news_items.append(f"ğŸ“° <b>{source['name']}</b>\nğŸ“Œ {title}\nğŸ”— {link}")
        
        return news_items[:3] if news_items else []
        
    except Exception as e:
        print(f"âŒ {source['name']} ç½‘ç«™æŠ“å–å¤±è´¥: {str(e)}")
        return []

# âœ… æ¸…ç†æ–‡æœ¬
def clean_text(text):
    # ç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\x20-\x7E\u4E00-\u9FFF]', '', text)
    return text.strip()

# âœ… Telegramæ¶ˆæ¯å‘é€
def send_telegram(message):
    try:
        payload = {
            "chat_id": TG_CHAT_ID,
            "text": message,
            "parse_mode": "HTML",
            "disable_web_page_preview": True
        }
        
        response = requests.post(API_URL, json=payload, timeout=20)
        response.raise_for_status()
        
        print(f"âœ… æ¶ˆæ¯å‘é€æˆåŠŸ: {message[:50]}...")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Telegram APIè¯·æ±‚å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ å‘é€æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return False

# âœ… ä¸»å‡½æ•°
def main():
    print("="*50)
    start_time = datetime.now()
    print(f"ğŸ“… æ–°é—»æ¨é€ä»»åŠ¡å¼€å§‹äº {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)
    
    # è·å–æ‰€æœ‰æ–°é—»æºçš„æœ€æ–°æ–°é—»
    all_news = []
    for source in NEWS_SOURCES:
        if "api_url" in source:
            news = fetch_api_news(source)
        elif "type" in source and source["type"] == "scrape":
            news = scrape_website(source)
        else:
            continue
            
        all_news.extend(news)
        time.sleep(1)  # è¯·æ±‚é—´å»¶è¿Ÿ
    
    # å¦‚æœæ²¡æœ‰æ–°é—»ï¼Œæ·»åŠ é»˜è®¤æ¶ˆæ¯
    if not all_news:
        all_news = ["ğŸ“¢ ä»Šæ—¥æš‚æ— æ–°é—»æ›´æ–°ï¼Œè¯·ç¨åå†è¯•"]
    
    print("\n" + "="*50)
    print(f"ğŸ“Š å…±è·å–åˆ° {len(all_news)} æ¡æ–°é—»:")
    for i, news in enumerate(all_news, 1):
        print(f"{i}. {news[:60]}{'...' if len(news) > 60 else ''}")
    
    # å‘é€æ–°é—»
    success_count = 0
    for news in all_news:
        max_retries = 2
        for attempt in range(max_retries):
            if send_telegram(news):
                success_count += 1
                time.sleep(1)  # æ¶ˆæ¯é—´é—´éš”
                break
            elif attempt < max_retries - 1:
                print(f"ç­‰å¾…3ç§’åé‡è¯• ({attempt+1}/{max_retries})...")
                time.sleep(3)
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "="*50)
    print(f"ğŸ ä»»åŠ¡å®Œæˆ! æˆåŠŸå‘é€ {success_count}/{len(all_news)} æ¡æ–°é—»")
    print(f"â±ï¸ å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸ ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸ æ€»è€—æ—¶: {duration.total_seconds():.1f}ç§’")
    print("="*50)
    
    # å¦‚æœæœ‰ä»»ä½•å¤±è´¥ï¼Œéé›¶é€€å‡ºç 
    if success_count < len(all_news):
        sys.exit(1)

if __name__ == "__main__":
    main()

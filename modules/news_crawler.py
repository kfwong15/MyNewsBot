# modules/news_crawler.py

import re
import json
import logging
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger('news_crawler')

BASE_DOMAIN    = "https://www.thestar.com.my"
LISTING_PAGE   = urljoin(BASE_DOMAIN, "/news/latest/")
OFFICIAL_RSS   = "https://www.thestar.com.my/rss/latest.rss"
GOOGLE_NEWS_RSS = (
    "https://news.google.com/rss/"
    "search?q=site:thestar.com.my/news/latest&hl=en-MY&gl=MY&ceid=MY:en"
)
MIN_COUNT      = 10
ARTICLE_REGEX  = re.compile(
    r'href="(/news/[^"]+?/\d{4}/\d{2}/\d{2}/[^"]+)"'
)


def fetch_rss(feed_url: str) -> list:
    """å°è¯•æŠ“å®˜æ–¹ RSS / Google News RSS"""
    try:
        resp = requests.get(feed_url, timeout=10)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
    except Exception as e:
        logger.warning(f"RSS è¯·æ±‚æˆ–è§£æå¤±è´¥: {feed_url} ï¼ {e}")
        return []

    items = []
    seen = set()
    # å®˜æ–¹å’Œ Google RSS éƒ½ç”¨ <item>
    for item in root.findall('.//item'):
        title = item.findtext('title', default='').strip()
        link  = item.findtext('link',  default='').strip()
        if not title or not link or link in seen:
            continue

        # å°è¯•ä» <description> ä¸­æå–å›¾ç‰‡ï¼ˆGoogle RSSï¼‰
        img = None
        desc = item.findtext('description', default='')
        if desc:
            soup = BeautifulSoup(desc, 'html.parser')
            img_tag = soup.find('img')
            if img_tag and img_tag.get('src'):
                img = img_tag['src']

        # å°è¯• <enclosure> æˆ– media:contentï¼ˆå®˜æ–¹ RSSï¼‰
        if not img:
            enc = item.find('enclosure')
            if enc is not None and 'url' in enc.attrib:
                img = enc.attrib['url']
            else:
                m = item.find('{http://search.yahoo.com/mrss/}content')
                if m is not None and 'url' in m.attrib:
                    img = m.attrib['url']

        items.append({"title": title, "link": link, "image": img})
        seen.add(link)
        if len(items) >= MIN_COUNT:
            break

    logger.info(f"âœ… RSS æŠ“åˆ° {len(items)} æ¡ ({feed_url})")
    return items


def fetch_jsonld() -> list:
    """ä» listing é¡µé¢é‡Œæ‰¾ JSON-LD ItemListï¼ˆåå¤‡ï¼‰"""
    try:
        resp = requests.get(LISTING_PAGE, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
    except Exception as e:
        logger.warning(f"JSON-LD é¡µé¢è¯·æ±‚å¤±è´¥: {e}")
        return []

    for script in soup.select('script[type="application/ld+json"]'):
        try:
            data = json.loads(script.string or '')
        except:
            continue
        if data.get('@type') == 'ItemList':
            out = []
            for ele in data.get('itemListElement', [])[:MIN_COUNT]:
                it    = ele.get('item', {})
                title = it.get('headline','').strip()
                link  = it.get('url','').strip()
                if not title or not link:
                    continue
                # image æœ‰æ—¶æ˜¯ str æœ‰æ—¶æ˜¯ dict
                raw = it.get('image')
                img = raw.get('url') if isinstance(raw, dict) else raw
                out.append({"title": title, "link": link, "image": img})
            logger.info(f"âœ… JSON-LD æŠ“åˆ° {len(out)} æ¡")
            return out
    logger.info("âš ï¸ æœªæ‰¾åˆ° JSON-LD ItemList")
    return []


def fetch_by_regex() -> list:
    """æ­£åˆ™æŠ½å– listing é¡µé¢æ‰€æœ‰æ–‡ç«  URLï¼Œå†è¯·æ±‚è¯¦æƒ…é¡µæŠ“ og:meta"""
    try:
        resp = requests.get(LISTING_PAGE, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        logger.warning(f"è¯¦æƒ…é¡µåˆ—è¡¨è¯·æ±‚å¤±è´¥: {e}")
        return []

    found = ARTICLE_REGEX.findall(resp.text)
    seen = []
    for p in found:
        if p not in seen:
            seen.append(p)
        if len(seen) >= MIN_COUNT:
            break

    out = []
    for path in seen:
        url = urljoin(BASE_DOMAIN, path)
        try:
            r2 = requests.get(url, timeout=10)
            r2.raise_for_status()
            sp = BeautifulSoup(r2.text, 'html.parser')
        except:
            continue

        # æŠ“æ ‡é¢˜
        title_tag = sp.find('meta', property='og:title')
        title = title_tag['content'].strip() if title_tag else (
            sp.find('h1').get_text(strip=True) if sp.find('h1') else ''
        )
        # æŠ“å›¾ç‰‡
        img_tag = sp.find('meta', property='og:image')
        img = img_tag['content'].strip() if img_tag else None

        if title and url:
            out.append({"title": title, "link": url, "image": img})

    logger.info(f"âœ… æ­£åˆ™+è¯¦æƒ…é¡µ æŠ“åˆ° {len(out)} æ¡")
    return out


def fetch_news() -> list:
    """
    æŠ“æ–°é—»ä¸»æµç¨‹ï¼šä¾æ¬¡å°è¯•
    1. å®˜æ–¹ RSS
    2. Google News RSS
    3. JSON-LD
    4. æ­£åˆ™+è¯¦æƒ…é¡µ
    """
    # 1) å®˜ç½‘ RSS
    out = fetch_rss(OFFICIAL_RSS)
    if len(out) < MIN_COUNT:
        # 2) Google News RSS
        more = fetch_rss(GOOGLE_NEWS_RSS)
        # åˆå¹¶å»é‡
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    if len(out) < MIN_COUNT:
        # 3) JSON-LD
        more = fetch_jsonld()
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    if len(out) < MIN_COUNT:
        # 4) æ­£åˆ™+è¯¦æƒ…é¡µ
        more = fetch_by_regex()
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    logger.info(f"ğŸ”š æœ€ç»ˆå…±æŠ“åˆ° {len(out)} æ¡æ–°é—»")
    return out


def select_random_news(news_list: list, count: int = 10) -> list:
    import random
    return random.sample(news_list, min(len(news_list), count))

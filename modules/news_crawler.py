# modules/news_crawler.py

import re
import json
import logging
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger('news_crawler')

BASE_DOMAIN     = "https://www.thestar.com.my"
LISTING_PAGE    = urljoin(BASE_DOMAIN, "/news/latest/")
OFFICIAL_RSS    = "https://www.thestar.com.my/rss/latest.rss"
GOOGLE_NEWS_RSS = (
    "https://news.google.com/rss/"
    "search?q=site:thestar.com.my/news/latest&hl=en-MY&gl=MY&ceid=MY:en"
)
MIN_COUNT       = 10
ARTICLE_REGEX   = re.compile(
    r'href="(/news/[^"]+?/\d{4}/\d{2}/\d{2}/[^"]+)"'
)


def fetch_rss(feed_url: str) -> list:
    """æŠ“å– RSSï¼ˆå®˜æ–¹æˆ– Google Newsï¼‰ï¼Œè¿”å›æœ€å¤š MIN_COUNT æ¡æ–°é—»é¡¹ã€‚"""
    items = []
    seen = set()
    try:
        resp = requests.get(feed_url, timeout=10)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
    except Exception as e:
        logger.warning(f"RSS è¯·æ±‚/è§£æå¤±è´¥: {feed_url} â€“ {e}")
        return items

    for item in root.findall('.//item'):
        title = item.findtext('title', '').strip()
        link  = item.findtext('link',  '').strip()
        if not title or not link or link in seen:
            continue

        # ä» <description> ä¸­æå–å›¾ç‰‡ï¼ˆGoogle News RSSï¼‰
        img = None
        desc = item.findtext('description', '')
        if desc:
            soup = BeautifulSoup(desc, 'html.parser')
            img_tag = soup.find('img')
            if img_tag and img_tag.get('src'):
                img = img_tag['src']

        # å®˜æ–¹ RSS ä¸­å¯èƒ½åŒ…å« <enclosure> æˆ– media:content
        if not img:
            enc = item.find('enclosure')
            if enc is not None and 'url' in enc.attrib:
                img = enc.attrib['url']
            else:
                m = item.find('{http://search.yahoo.com/mrss/}content')
                if m is not None and 'url' in m.attrib:
                    img = m.attrib['url']

        items.append({"title": title, "link": link, "image": img})
        seen.add(link)
        if len(items) >= MIN_COUNT:
            break

    logger.info(f"âœ… RSS æŠ“åˆ° {len(items)} æ¡ ({feed_url})")
    return items


def fetch_jsonld() -> list:
    """è§£æåˆ—è¡¨é¡µä¸­çš„ JSON-LD ItemListï¼Œä½œä¸º RSS å¤±è´¥åçš„åå¤‡ã€‚"""
    out = []
    try:
        resp = requests.get(LISTING_PAGE, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
    except Exception as e:
        logger.warning(f"JSON-LD é¡µé¢è¯·æ±‚å¤±è´¥: {e}")
        return out

    for script in soup.select('script[type="application/ld+json"]'):
        try:
            data = json.loads(script.string or '')
        except:
            continue
        if isinstance(data, dict) and data.get('@type') == 'ItemList':
            for ele in data.get('itemListElement', [])[:MIN_COUNT]:
                it    = ele.get('item', {})
                title = it.get('headline','').strip()
                link  = it.get('url','').strip()
                if not title or not link:
                    continue
                raw = it.get('image')
                img = raw.get('url') if isinstance(raw, dict) else raw
                out.append({"title": title, "link": link, "image": img})
            logger.info(f"âœ… JSON-LD æŠ“åˆ° {len(out)} æ¡")
            return out

    logger.info("âš ï¸ æœªæ‰¾åˆ° JSON-LD ItemList")
    return out


def fetch_by_regex() -> list:
    """ç”¨æ­£åˆ™æŠ½å–åˆ—è¡¨é¡µæ–‡ç«  URLï¼Œå†è¯·æ±‚è¯¦æƒ…é¡µæŠ“ og:title å’Œ og:imageã€‚"""
    out = []
    try:
        resp = requests.get(LISTING_PAGE, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        logger.warning(f"åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return out

    paths = ARTICLE_REGEX.findall(resp.text)
    seen = []
    for p in paths:
        if p not in seen:
            seen.append(p)
        if len(seen) >= MIN_COUNT:
            break

    for path in seen:
        url = urljoin(BASE_DOMAIN, path)
        try:
            r2 = requests.get(url, timeout=10)
            r2.raise_for_status()
            sp = BeautifulSoup(r2.text, 'html.parser')
        except Exception:
            continue

        # æ ‡é¢˜ä¼˜å…ˆå– OpenGraph å…ƒæ ‡ç­¾
        tag_t = sp.find('meta', property='og:title')
        title = (tag_t['content'].strip() if tag_t and tag_t.get('content') 
                 else sp.find('h1').get_text(strip=True) if sp.find('h1') 
                 else '')
        # å›¾ç‰‡ä¼˜å…ˆå– OpenGraph å…ƒæ ‡ç­¾
        tag_i = sp.find('meta', property='og:image')
        img = tag_i['content'].strip() if tag_i and tag_i.get('content') else None

        if title and url:
            out.append({"title": title, "link": url, "image": img})

    logger.info(f"âœ… æ­£åˆ™+è¯¦æƒ…æŠ“åˆ° {len(out)} æ¡")
    return out


def get_og_image(url: str) -> str | None:
    """å•ç¯‡æ–‡ç« è¯¦æƒ…é¡µè¡¥æŠ“ OpenGraph å›¾ç‰‡ï¼Œç¡®ä¿æ¯æ¡æ–°é—»æœ‰å›¾ã€‚"""
    try:
        r = requests.get(url, timeout=5)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        tag = soup.find('meta', property='og:image')
        if tag and tag.get('content'):
            return tag['content']
    except Exception as e:
        logger.warning(f"è·å– og:image å¤±è´¥: {url} â€“ {e}")
    return None


def fetch_news() -> list:
    """
    ä¸»æŠ“å–æµç¨‹ï¼š
    1. å®˜æ–¹ RSS
    2. Google News RSS
    3. JSON-LD
    4. æ­£åˆ™æŠ½ URL + è¯¦æƒ…é¡µ
    æœ€åå¯¹æ— å›¾æ–°é—»ä¸€ä¸€è¡¥æŠ“ OpenGraph å›¾ç‰‡ã€‚
    """
    out = fetch_rss(OFFICIAL_RSS)
    if len(out) < MIN_COUNT:
        more = fetch_rss(GOOGLE_NEWS_RSS)
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    if len(out) < MIN_COUNT:
        more = fetch_jsonld()
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    if len(out) < MIN_COUNT:
        more = fetch_by_regex()
        links = {n['link'] for n in out}
        for m in more:
            if len(out) >= MIN_COUNT:
                break
            if m['link'] not in links:
                out.append(m)

    # è¡¥æŠ“ç¼ºå¤±çš„ OG å›¾
    for item in out:
        if not item.get('image'):
            item['image'] = get_og_image(item['link'])

    logger.info(
        f"ğŸ”š æœ€ç»ˆå…±æŠ“åˆ° {len(out)} æ¡æ–°é—»ï¼Œå¸¦å›¾ {sum(1 for i in out if i.get('image'))} æ¡"
    )
    return out


def select_random_news(news_list: list, count: int = 10) -> list:
    """ä»æŠ“åˆ°çš„æ–°é—»ä¸­éšæœºé€‰å– count æ¡ã€‚"""
    import random
    return random.sample(news_list, min(len(news_list), count))

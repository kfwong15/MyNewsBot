import requests
from bs4 import BeautifulSoup
import random
import logging
from datetime import datetime
import re
import sqlite3
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('news_crawler')

# æ–°é—»åˆ†ç±»URL
NEWS_CATEGORIES = {
    'latest': 'https://www.thestar.com.my/news',
    'nation': 'https://www.thestar.com.my/news/nation',
    'politics': 'https://www.thestar.com.my/news/politics',
    'business': 'https://www.thestar.com.my/business',
    'sport': 'https://www.thestar.com.my/sport',
    'entertainment': 'https://www.thestar.com.my/entertainment',
    'tech': 'https://www.thestar.com.my/tech',
    'lifestyle': 'https://www.thestar.com.my/lifestyle'
}

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
HEADERS = {'User-Agent': USER_AGENT}

class NewsDatabase:
    def __init__(self, db_path='news.db'):
        self.conn = sqlite3.connect(db_path)
        self._create_table()
    
    def _create_table(self):
        self.conn.execute('''CREATE TABLE IF NOT EXISTS sent_news
                           (link TEXT PRIMARY KEY, sent_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
        self.conn.commit()
    
    def is_duplicate(self, link):
        cursor = self.conn.execute("SELECT 1 FROM sent_news WHERE link=?", (link,))
        return cursor.fetchone() is not None
    
    def mark_as_sent(self, link):
        try:
            self.conn.execute("INSERT OR IGNORE INTO sent_news (link) VALUES (?)", (link,))
            self.conn.commit()
        except sqlite3.IntegrityError:
            pass  # å¿½ç•¥é‡å¤æ’å…¥
    
    def cleanup_old_entries(self, days=7):
        """ç§»é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ¡ç›®"""
        self.conn.execute("DELETE FROM sent_news WHERE sent_time < datetime('now', ?)", (f'-{days} days',))
        self.conn.commit()

def fetch_news():
    """ä»TheStaræŠ“å–æ–°é—»"""
    db = NewsDatabase()
    all_news = []
    
    for category, url in NEWS_CATEGORIES.items():
        try:
            logger.info(f"æŠ“å–åˆ†ç±»: {category}")
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ ¹æ®å®é™…ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
            articles = soup.select('div.timeline-item')[:20]  # æ¯ç±»æœ€å¤š20æ¡
            
            for article in articles:
                try:
                    title_elem = article.select_one('h2 a, h3 a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.text.strip()
                    link = title_elem.get('href', '')
                    if link and not link.startswith('http'):
                        link = f'https://www.thestar.com.my{link}'
                    
                    # è·³è¿‡é‡å¤
                    if db.is_duplicate(link):
                        continue
                    
                    img_elem = article.select_one('img')
                    img_url = img_elem.get('src', '') if img_elem else ''
                    
                    summary_elem = article.select_one('p')
                    summary = summary_elem.text.strip() if summary_elem else ""
                    
                    time_elem = article.select_one('div.timestamp, time')
                    time_str = time_elem.text.strip() if time_elem else datetime.now().strftime('%Y-%m-%d')
                    
                    # æ¸…ç†æ—¶é—´æ ¼å¼
                    time_str = re.sub(r'\s+', ' ', time_str)
                    
                    all_news.append({
                        'title': title,
                        'link': link,
                        'img_url': img_url,
                        'summary': summary,
                        'time': time_str,
                        'category': category
                    })
                except Exception as e:
                    logger.error(f"è§£ææ–‡ç« å¤±è´¥: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"æŠ“å–åˆ†ç±» {category} å¤±è´¥: {e}", exc_info=True)
    
    # æ¸…ç†æ—§æ¡ç›®
    db.cleanup_old_entries()
    
    return all_news

def select_random_news(news_list, min_news=30, max_news=50):
    """éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„æ–°é—»"""
    if not news_list:
        return []
    
    num_news = random.randint(min_news, min(max_news, len(news_list)))
    return random.sample(news_list, num_news)

def format_news_message(news):
    """æ ¼å¼åŒ–æ–°é—»ä¸ºTelegramæ¶ˆæ¯"""
    return (
        f"ğŸ”¹ *{news['title']}*\n"
        f"â° {news['time']}\n"
        f"{news['summary']}\n"
        f"[é˜…è¯»å…¨æ–‡]({news['link']})"
    )

if __name__ == "__main__":
    # æµ‹è¯•çˆ¬è™«
    news = fetch_news()
    print(f"æŠ“å–åˆ° {len(news)} æ¡æ–°é—»")
    for i, item in enumerate(news[:3], 1):
        print(f"\n--- æ–°é—» {i} ---")
        print(f"æ ‡é¢˜: {item['title']}")
        print(f"åˆ†ç±»: {item['category']}")
        print(f"é“¾æ¥: {item['link']}")

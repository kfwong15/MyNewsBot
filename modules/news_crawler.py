# modules/news_crawler.py

import logging
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from urllib.parse import urljoin

logger = logging.getLogger('news_crawler')

# åªç”¨ Google News RSS å¼ºåˆ¶æ‹¿åˆ°è‡³å°‘ 10 æ¡
GOOGLE_NEWS_RSS = (
    "https://news.google.com/rss/"
    "search?q=site:thestar.com.my/news/latest&hl=en-MY&gl=MY&ceid=MY:en"
)
BASE_DOMAIN = "https://www.thestar.com.my"
MIN_COUNT = 10

def fetch_google_rss() -> list[dict]:
    """
    è°ƒç”¨ Google News RSSï¼Œæ‹¿æœ€æ–° /news/latest ä¸‹çš„æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜ã€‚
    æœ€å¤šè¿”å› MIN_COUNT æ¡ã€‚
    """
    items = []
    try:
        resp = requests.get(GOOGLE_NEWS_RSS, timeout=10)
        resp.raise_for_status()
        root = ET.fromstring(resp.content)
    except Exception as e:
        logger.error(f"Google RSS è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
        return items

    for node in root.findall('.//item'):
        title = node.findtext('title', default='').strip()
        link  = node.findtext('link',  default='').strip()
        if title and link:
            items.append({"title": title, "link": link})
        if len(items) >= MIN_COUNT:
            break

    logger.info(f"âœ… Google RSS æŠ“åˆ° {len(items)} æ¡")
    return items


def get_article_details(url: str) -> tuple[str, str | None, str]:
    """
    è¯·æ±‚æ–‡ç« è¯¦æƒ…é¡µï¼Œè§£æ:
      - æ ‡é¢˜: <meta property="og:title"> æˆ– <h1>
      - å›¾ç‰‡:  <meta property="og:image">
      - å†…å®¹:  ç¬¬ä¸€ä¸ªé•¿åº¦ >50 çš„ <p> ä½œä¸ºæ‘˜è¦
    """
    title = ""
    image = None
    content = ""

    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        # æ ‡é¢˜
        ogt = soup.find('meta', property='og:title')
        if ogt and ogt.get('content'):
            title = ogt['content'].strip()
        else:
            h1 = soup.find('h1')
            title = h1.get_text(strip=True) if h1 else ""

        # å›¾ç‰‡
        ogi = soup.find('meta', property='og:image')
        if ogi and ogi.get('content'):
            image = ogi['content'].strip()

        # å†…å®¹æ‘˜è¦
        for p in soup.find_all('p'):
            txt = p.get_text(strip=True)
            if len(txt) > 50:
                content = txt
                break

    except Exception as e:
        logger.warning(f"è¯¦æƒ…è§£æå¤±è´¥ {url}: {e}")

    return title, image, content


def fetch_news() -> list[dict]:
    """
    ä¸»è°ƒç”¨ï¼šå…ˆä» Google RSS æ‹‰æ ‡é¢˜å’Œé“¾æ¥ï¼Œ
    å†é€ç¯‡è¯·æ±‚è¯¦æƒ…é¡µè¡¥é½æ ‡é¢˜ã€é…å›¾å’Œå†…å®¹æ‘˜è¦ã€‚
    """
    raw = fetch_google_rss()
    news = []

    for item in raw:
        title, img, snippet = get_article_details(item['link'])
        # å¦‚æœè¯¦æƒ…é¡µæ²¡æœ‰æŠ“åˆ°æ ‡é¢˜ï¼Œç”¨ RSS æ ‡é¢˜å…œåº•
        final_title = title or item['title']
        news.append({
            "title": final_title,
            "link": item['link'],
            "image": img,
            "content": snippet
        })

    logger.info(f"ğŸ”š æœ€ç»ˆç»„è£… {len(news)} æ¡æ–°é—»")
    return news


def select_random_news(news_list: list[dict], count: int = 10) -> list[dict]:
    """
    ä»åˆ—è¡¨ä¸­éšæœºé€‰è‹¥å¹²æ¡æ–°é—»ï¼ˆæˆ–å…¨é€‰ï¼‰ã€‚
    """
    import random
    return random.sample(news_list, min(len(news_list), count))
